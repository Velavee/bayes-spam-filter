{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed19cfc7",
   "metadata": {},
   "source": [
    "# Bayes Theorem for Predicting the Probability of an Email Being Spam\n",
    "\n",
    "S = Spam\n",
    "w = Word\n",
    "\n",
    "$P(Spam|w_{1}, w_{2},..., w_{n}) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_{i}|Spam)$\n",
    "\n",
    "The probability that an email consisting of the words $w_{1}, w_{2},... w_{n}$ is proportional to the probability that any given email is spam multiplied by the product of each word's probability to appear in a spam email.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83be745",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42190751",
   "metadata": {},
   "source": [
    "## Step 1: Partition the data into training and test segments\n",
    "\n",
    "20% of the data for testing, and the remaining 80% is training (i.e. the 80% training data will confirm whether the 20% testing data labels are correct)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e254a",
   "metadata": {},
   "source": [
    "## Step 2: Get probabilities that any one email in the training data is either spam or ham\n",
    "\n",
    "In the labelled dataset, count the number of spam and ham emails.\n",
    "\n",
    "$P(Spam) = \\frac{Spam\\,Emails}{Total\\,Emails}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bca9ca",
   "metadata": {},
   "source": [
    "$P(Ham) = \\frac{Ham\\,Emails}{Total\\,Emails}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e43c9",
   "metadata": {},
   "source": [
    "## Step 3: Get the \"spamicity\" and \"hamicity\" probability of each word in the testing data email\n",
    "\n",
    "**w** = word\n",
    "<br>**vocab** = total words in dataset\n",
    "<br>**spam_vocab**\n",
    "<br>**wi_spam_count**\n",
    "\n",
    "Count all unique words in the labelled dataset to get **vocab**.\n",
    "\n",
    "Count the total number of words in labelled spam emails (ignoring uniqueness) to get **spam_vocab**.\n",
    "\n",
    "For each word **w**, count all instances of the word in the spam emails to get **wi_spam_count**.\n",
    "\n",
    "Calculate spamicity of each word and store the word and its spamicity in a dictionary\n",
    "\n",
    "$P(w_{i}|Spam) = \\frac{wi\\_spam\\_count\\,+\\,\\alpha}{spam\\_vocab\\,+\\,\\alpha \\cdot vocab}$\n",
    "\n",
    "$\\alpha$ is a coefficient that prevents a probability from being 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba26950",
   "metadata": {},
   "source": [
    "## Step 4: Calculate the \"spamicity\" and \"hamicity\" of each email\n",
    "\n",
    "Multiply spamicities of each word together to get $\\prod_{i=1}^{n}P(w_{i}|Spam)$.\n",
    "\n",
    "Multiply that product by the probability that any email is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98189d84",
   "metadata": {},
   "source": [
    "## Step 5: Compare hamicity and spamicity scores to classify emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728084db",
   "metadata": {},
   "source": [
    "## Step 6: Check accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68506f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "PREDICTION = 'Prediction'\n",
    "CLASSIFICATION = 'Classiciation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c127b8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c440a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_confusion_matrix(data):\n",
    "    confusion_matrix = {\n",
    "        'email_#': list(range(1,data.shape[0]+1)),\n",
    "        'actual_classification': data[PREDICTION].values.tolist(),\n",
    "        'predicted_classification': [],\n",
    "        'result': []\n",
    "    }\n",
    "    \n",
    "    for index, email in data.iterrows():\n",
    "        actual = email[PREDICTION]\n",
    "        prediction = email[CLASSIFICATION]\n",
    "        \n",
    "        confusion_matrix['predicted_classification'].append(prediction)\n",
    "        \n",
    "        if actual and prediction:\n",
    "            confusion_matrix['result'].append('TP')\n",
    "        elif not actual and not prediction:\n",
    "            confusion_matrix['result'].append('TN')\n",
    "        elif actual and not prediction:\n",
    "            confusion_matrix['result'].append('FN')\n",
    "        else:\n",
    "            confusion_matrix['result'].append('FP')\n",
    "            \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07fc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_input_experiment(data):\n",
    "    experiment_results = { 'inputs': [], 'avg_accuracy': [], 'avg_time':[], 'TP': [], 'TN': [], 'FP': [], 'FN': [] }\n",
    "    \n",
    "    for i in range(100,3001,100):\n",
    "        email_subset = data.sample(n = i)\n",
    "\n",
    "        nb_subset = NB_Model(email_subset)\n",
    "        subset_run = nb_subset.run_partitions()\n",
    "        subset_run_table = pd.DataFrame(data = nb_subset.time_accuracy_results)\n",
    "        \n",
    "        experiment_results['inputs'].append(i)\n",
    "        experiment_results['avg_accuracy'].append(subset_run_table['accuracy'].mean())\n",
    "        experiment_results['avg_time'].append(subset_run_table['time_elapsed'].mean())\n",
    "        experiment_results['TP'].append(len(subset_run[subset_run.Confusion == 'TP']))\n",
    "        experiment_results['TN'].append(len(subset_run[subset_run.Confusion == 'TN']))\n",
    "        experiment_results['FP'].append(len(subset_run[subset_run.Confusion == 'FP']))\n",
    "        experiment_results['FN'].append(len(subset_run[subset_run.Confusion == 'FN']))\n",
    "\n",
    "    return pd.DataFrame(data = experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16d35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB_Model:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.time_accuracy_results = {'partition': [], 'accuracy': [], 'time_elapsed': []}\n",
    "        \n",
    "    def consolidate_training_data(self, begin, end, i):\n",
    "        if i == 1:\n",
    "                training_data = self.data.iloc[end:]\n",
    "        elif i == 5:\n",
    "            training_data = self.data.iloc[:begin]\n",
    "        else:\n",
    "            training_data_sections = []\n",
    "            training_data_sections.append(self.data.iloc[:begin])\n",
    "            training_data_sections.append(self.data.iloc[end:])\n",
    "            training_data = pd.concat(training_data_sections)\n",
    "            \n",
    "        return training_data\n",
    "    \n",
    "    def count_vocab(self, emails):\n",
    "        total_words = 0\n",
    "\n",
    "        for index, row in emails.iterrows():\n",
    "            total_words += sum(row.values[1:-2])\n",
    "\n",
    "        return total_words\n",
    "    \n",
    "    def calculate_word_spamicity(self, w_spam_count, vocab, spam_vocab):\n",
    "        alpha = 1\n",
    "\n",
    "        spamicity = (w_spam_count + alpha) / (spam_vocab + alpha * vocab)\n",
    "        return spamicity\n",
    "    \n",
    "    def build_word_spamicity_dict(self, spam_emails, vocab, spam_vocab):\n",
    "        spam_word_appearances = {}\n",
    "\n",
    "        for (column_name, column_data) in spam_emails.iteritems():\n",
    "            if column_name != 'Email No.' and column_name != PREDICTION and column_name != CLASSIFICATION and column_name != 'Confusion':\n",
    "                spam_word_appearances[column_name] = sum(column_data.values)\n",
    "\n",
    "        for word in spam_word_appearances:\n",
    "            spam_word_appearances[word] = self.calculate_word_spamicity(spam_word_appearances[word], vocab, spam_vocab)\n",
    "\n",
    "        return spam_word_appearances\n",
    "    \n",
    "    def calculate_email(self, email, word_spamicities, word_hamicities, spam_proportion, ham_proportion, testing_data):\n",
    "        email_spamicity = math.log(spam_proportion)\n",
    "        email_hamicity = math.log(ham_proportion)\n",
    "\n",
    "        for column in testing_data.columns[1:-3]:\n",
    "            if email[column] > 0:\n",
    "                email_spamicity += math.log(word_spamicities[column])*email[column]\n",
    "                email_hamicity += math.log(word_hamicities[column])*email[column]\n",
    "\n",
    "        return 1 if email_spamicity >= email_hamicity else 0\n",
    "    \n",
    "    def calc_cm_value(self, predicted, actual):\n",
    "        if actual and predicted:\n",
    "            return 'TP'\n",
    "        elif not actual and not predicted:\n",
    "            return 'TN'\n",
    "        elif actual and not predicted:\n",
    "            return 'FN'\n",
    "        else:\n",
    "            return 'FP'\n",
    "    \n",
    "    def calculate_accuracy(self, testing_emails):\n",
    "        number_correct = 0\n",
    "        for index, email in testing_emails.iterrows():\n",
    "            if email[PREDICTION] == email[CLASSIFICATION]:\n",
    "                number_correct += 1\n",
    "\n",
    "        return number_correct / testing_emails.shape[0] * 100\n",
    "    \n",
    "    def run_partitions(self, include_stop_words=True):\n",
    "        end = 0\n",
    "        begin = 0\n",
    "        total_num_emails = self.data.shape[0]\n",
    "        partition_size = total_num_emails//5\n",
    "        self.data[CLASSIFICATION] = \"\"\n",
    "        self.data['Confusion'] = ''\n",
    "        \n",
    "        # Subtract 2 for \"Email No.\" and \"Prediction\" columns\n",
    "        total_vocab = len(self.data.columns) - 2\n",
    "        \n",
    "        if not include_stop_words:\n",
    "            # Take out all stopwords\n",
    "            for col in df.columns:\n",
    "                if col in stop_words:\n",
    "                    self.data.drop(col, axis=1, inplace=True)\n",
    "                    \n",
    "        for i in range(1,6):\n",
    "            self.time_accuracy_results['partition'].append(i)\n",
    "            start_time = time.time()\n",
    "            end += partition_size\n",
    "            \n",
    "            if i != 5:\n",
    "                testing_data = self.data.iloc[begin:end]\n",
    "            else:\n",
    "                testing_data = self.data.iloc[begin:]\n",
    "\n",
    "            training_data = self.consolidate_training_data(begin, end, i)\n",
    "            \n",
    "            begin += partition_size\n",
    "\n",
    "            spam_proportion = training_data[PREDICTION].value_counts()[1] / training_data.shape[0]\n",
    "            ham_proportion = training_data[PREDICTION].value_counts()[0] / training_data.shape[0]\n",
    "\n",
    "            spam_training_emails = training_data.loc[training_data[PREDICTION] == 1]\n",
    "            total_spam_words = self.count_vocab(spam_training_emails)\n",
    "\n",
    "            ham_training_emails = training_data.loc[training_data[PREDICTION] == 0]\n",
    "            total_ham_words = self.count_vocab(ham_training_emails)\n",
    "\n",
    "            word_spamicities = self.build_word_spamicity_dict(spam_training_emails, total_vocab, total_spam_words)\n",
    "            word_hamicities = self.build_word_spamicity_dict(ham_training_emails, total_vocab, total_ham_words)\n",
    "            \n",
    "            j = 0\n",
    "            for index, email in testing_data.iterrows():\n",
    "                result = self.calculate_email(email, word_spamicities, word_hamicities, spam_proportion, ham_proportion, testing_data)\n",
    "                testing_data[CLASSIFICATION].loc[testing_data.index[j]] = result\n",
    "                self.data[CLASSIFICATION].loc[index] = result\n",
    "                self.data['Confusion'].loc[index] = self.calc_cm_value(result, self.data[PREDICTION].loc[index])\n",
    "                j += 1\n",
    "                \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            self.time_accuracy_results['time_elapsed'].append(elapsed_time)\n",
    "                \n",
    "            score = self.calculate_accuracy(testing_data)\n",
    "            self.time_accuracy_results['accuracy'].append(score)\n",
    "            \n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84227af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 29.0%\n",
      "Ham: 71.0%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('emails.csv')\n",
    "\n",
    "spam_proportion = df[PREDICTION].value_counts()[1] / df.shape[0]\n",
    "ham_proportion = df[PREDICTION].value_counts()[0] / df.shape[0]\n",
    "\n",
    "print(f'Spam: {round(spam_proportion * 100, 2)}%')\n",
    "print(f'Ham: {round(ham_proportion * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6777e",
   "metadata": {},
   "source": [
    "## Run Model with Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_with_stopwords = NB_Model(df)\n",
    "classified_data_1 = nb_with_stopwords.run_partitions(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72647c",
   "metadata": {},
   "source": [
    "## With Stopwords Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_stopwords_table = pd.DataFrame(data = nb_with_stopwords.time_accuracy_results)\n",
    "with_stopwords_table['with_stopwords'] = [True] * with_stopwords_table.shape[0]\n",
    "with_stopwords_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97590231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TP: {len(classified_data_1[classified_data_1.Confusion == \"TP\"])}')\n",
    "print(f'TN: {len(classified_data_1[classified_data_1.Confusion == \"TN\"])}')\n",
    "print(f'FP: {len(classified_data_1[classified_data_1.Confusion == \"FP\"])}')\n",
    "print(f'FN: {len(classified_data_1[classified_data_1.Confusion == \"FN\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe3b37",
   "metadata": {},
   "source": [
    "## Run Model Without Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_wo_stopwords = NB_Model(df)\n",
    "classified_data_2 = nb_wo_stopwords.run_partitions(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501ce54",
   "metadata": {},
   "source": [
    "## Without Stopwords Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_stopwords_table = pd.DataFrame(data = nb_wo_stopwords.time_accuracy_results)\n",
    "wo_stopwords_table['with_stopwords'] = [False] * wo_stopwords_table.shape[0]\n",
    "wo_stopwords_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TP: {len(classified_data_2[classified_data_2.Confusion == \"TP\"])}')\n",
    "print(f'TN: {len(classified_data_2[classified_data_2.Confusion == \"TN\"])}')\n",
    "print(f'FP: {len(classified_data_2[classified_data_2.Confusion == \"FP\"])}')\n",
    "print(f'FN: {len(classified_data_2[classified_data_2.Confusion == \"FN\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da22d62",
   "metadata": {},
   "source": [
    "## Analytics Comparing Runs With and Without Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([with_stopwords_table, wo_stopwords_table])\n",
    "\n",
    "accuracy_graph = all_results.groupby('with_stopwords')['accuracy'].mean().plot.bar()\n",
    "accuracy_graph.bar_label(accuracy_graph.containers[0])\n",
    "plt.title('Average Accuracy With and Without Stopwords')\n",
    "plt.ylabel('Average % Accurate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2513f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_graph = all_results.groupby('with_stopwords')['time_elapsed'].mean().plot.bar()\n",
    "time_graph.bar_label(time_graph.containers[0])\n",
    "plt.title('Average Runtimes With and Without Stopwords')\n",
    "plt.ylabel('Average Time in Seconds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a150e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_experiment_results = perform_input_experiment(df)\n",
    "input_experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(input_experiment_results['inputs'], input_experiment_results['avg_accuracy'])\n",
    "plt.grid(axis='y')\n",
    "plt.xlabel('# of Inputs')\n",
    "plt.ylabel('Average % Accurate')\n",
    "plt.title('Average Accuracy by # of Inputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a29efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(input_experiment_results['inputs'], input_experiment_results['FN'], label='FN')\n",
    "plt.plot(input_experiment_results['inputs'], input_experiment_results['FP'], label='FP')\n",
    "plt.plot(input_experiment_results['inputs'], input_experiment_results['TN'], label='TN')\n",
    "plt.plot(input_experiment_results['inputs'], input_experiment_results['TP'], label='TP')\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.xlabel('# of Inputs')\n",
    "plt.ylabel('# Classifications')\n",
    "plt.title('Confusion Matrix Classifications by # Inputs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
